{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd3924a-df5e-489a-bad6-b955efdfb58d",
   "metadata": {},
   "source": [
    "## This notebook runs the Ring All Reduce (RAR) architecture _with_ validation partition of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2ae2b-a9ae-4e83-9847-9a6b83b2fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Operating system functionalities\n",
    "import numpy as np  # Numerical operations and array handling\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Data visualization\n",
    "import pickle  # Serialization and deserialization of Python objects\n",
    "import random as rn  # Random number generator\n",
    "import itertools  # Functions for creating iterators for efficient looping\n",
    "from IPython.display import display  # Function for rendering in Jupyter Notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # Splitting data into training and testing sets\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scaling features to a range\n",
    "\n",
    "import tensorflow as tf  # Main machine learning framework\n",
    "from tensorflow.keras.layers import BatchNormalization  # Normalizing layer in neural networks\n",
    "import tensorflow.keras as keras  # High-level neural networks API\n",
    "from keras import regularizers  # Regularization techniques in Keras\n",
    "from keras.layers import Dropout  # Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b1752-26db-4e10-8e39-cbf8b048ad1e",
   "metadata": {},
   "source": [
    "Seeds to control randomness in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f708303-91fe-40b4-9c87-064481733991",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # Setting the random seed for reproducibility\n",
    "rn.seed(0)  # Setting the random seed for reproducibility\n",
    "tf.random.set_seed(0) # Setting the TensorFlow random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1639084-000b-449c-a36d-03c6510b61cf",
   "metadata": {},
   "source": [
    "Loading processed and curated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbea8a9-6bc6-48fa-95d5-248703c64a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeart = pd.read_csv(\"/srv/heart_ConditionalMeanImputation.csv\")\n",
    "dfHeart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe966b-c2ef-4dfe-9e26-abdfa5fa3a98",
   "metadata": {},
   "source": [
    "Assigning the name of the originating hospital to each point in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412e6f8-3566-45bd-b98b-2cafb8be23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeart[\"Name\"]= \"name\"\n",
    "# Sliced from original raw data\n",
    "dfHeart.loc[0:293,\"Name\"] = \"hung\" # Hungarian Institute of Cardiology, Budapest\n",
    "# Author: Andras Janosi, M.D.\n",
    "dfHeart.loc[293:477,\"Name\"] = \"swit\" # University Hospital (Zurich and Basel) Switzerland\n",
    "# Author: William Steinbrunn, M.D. and Matthias Pfisterer, M.D respectively.\n",
    "dfHeart.loc[477:615,\"Name\"] = \"long\" # V.A. Medical Center, Long Beach\n",
    "# Author: Robert Detrano, M.D., Ph.D.\n",
    "dfHeart.loc[615:766,\"Name\"] = \"stat\" # Statlog dataset (confidential source)\n",
    "# Author: \"King RD. Department of Statistics and Modelling Science, University of Strathclyde, Glasgow\"\n",
    "dfHeart.loc[766:918,\"Name\"] = \"clev\" # Cleveland Clinic Foundation\n",
    "# Author: Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1cbbb-47f1-4612-aa96-eb1880979a92",
   "metadata": {},
   "source": [
    "Creating different clients with the sliced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642f51e-e52e-4ec0-a431-1b1fad8eda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "groups = dfHeart.groupby('Name')\n",
    "\n",
    "for idx, group in enumerate(groups):\n",
    "    X = group[1].drop(['HeartDisease', 'Name'], axis=1)\n",
    "    y = group[1]['HeartDisease']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "    \n",
    "    # Scaling the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Creating a dictionary for each client\n",
    "    data_dict[idx] = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64587551-4c37-4bd5-b221-2f03f0fe65d0",
   "metadata": {},
   "source": [
    "Building the optimal ANN architecture, previously estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc50193-f08e-444a-880c-3a1af9e97308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "shape = data_dict[list(data_dict.keys())[0]]['X_train'].shape[1]\n",
    "\n",
    "# Input layer\n",
    "model.add(keras.layers.Dense(units=256, input_shape=(shape,), activation=\"relu\", kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Add a Dropout layer with a dropout rate of 50%\n",
    "\n",
    "# Hidden layers\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Add a Dropout layer with a dropout rate of 50%\n",
    "\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Add a Dropout layer with a dropout rate of 50%\n",
    "\n",
    "model.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Add a Dropout layer with a dropout rate of 50%\n",
    "\n",
    "model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))  # Add a Dropout layer with a dropout rate of 50%\n",
    "\n",
    "# Output layer\n",
    "model.add(keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\", \"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d520be-9e72-4243-91a1-b3fd7f23b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ave_weights(n_i,listOfWeights):\n",
    "    \"\"\"\n",
    "    Aggregation function\n",
    "\n",
    "   :param list n_i: Number of samples for each client\n",
    "   :param list listOfWeights: Weights for each client\n",
    "   :return: Final weighted average for global model\n",
    "    \"\"\"\n",
    "    N = sum(n_i) # total number of samples of all clients\n",
    "    # initial weights of global model, set to zero\n",
    "    ave_weights = listOfWeights[0]\n",
    "    ave_weights = [i * 0 for i in ave_weights]\n",
    "    # loop whose range is number of clients\n",
    "    for j in range(len(n_i)):\n",
    "        # receive weights from clients\n",
    "        rec_weight = listOfWeights[j]\n",
    "        # multiply the client weights by number of local data samples in client local data\n",
    "        rec_weight =  [i * n_i[j] for i in rec_weight]\n",
    "        # divide the weights by total number of samples of all clients\n",
    "        rec_weight =  [i / N for i in rec_weight]\n",
    "        # sum the weights of new client with the prior\n",
    "        ave_weights = [x + y for x, y in zip(ave_weights,rec_weight)]\n",
    "    return ave_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fc33f-a3a1-444f-8e82-459766cd2062",
   "metadata": {},
   "source": [
    "Ring all reduce architecture with 5 clients: [5,1,2,3,4] -> [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169753d5-0a47-43ab-b2de-e908ba22fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_send = list(range(1,len(data_dict)+1))\n",
    "clients_send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f998d07-cb58-42c0-a581-586b54bf6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = clients_send.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca770d5d-57b4-42a1-9cd8-98cac4b8d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.insert(0, clients.pop())\n",
    "print(clients) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0956cdc-ea42-4479-8829-594a778de4b5",
   "metadata": {},
   "source": [
    "The weights are sent from clients to clients_send: [5,1,2,3,4] -> [1,2,3,4,5] <br>\n",
    "Client 1 will possess a model (formed by aggregating the weights of clients 5 and 1), <br>\n",
    "which will differ from client 2 (formed by the weights of clients 1 and 2) <br>\n",
    "client_send[i] could even be a list of clients <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7014d-8641-4963-a7f3-4162aab56df5",
   "metadata": {},
   "source": [
    "Initialization of empty lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e26ce-cf63-4e2d-8061-d77b33a37a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for validation\n",
    "loss_val_round = [[] for _ in range(len(clients))] \n",
    "acc_val_round = [[] for _ in range(len(clients))]  \n",
    "AUC_val_round = [[] for _ in range(len(clients))] \n",
    "# Metrics for test\n",
    "acc_test_round = [[] for _ in range(len(clients))] \n",
    "loss_test_round = [[] for _ in range(len(clients))]\n",
    "AUC_test_round = [[] for _ in range(len(clients))]\n",
    "# To save client weights of certain rounds\n",
    "save_weights = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96849862-88a9-496b-a511-afc6a32efbdb",
   "metadata": {},
   "source": [
    "Running the rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e5a9c-6dee-4208-a997-f86d4e1ae648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of rounds\n",
    "n_times = 100\n",
    "\n",
    "for i in range(1,n_times+1):\n",
    "    # Each round begins with one weight per client.\n",
    "    # weights_clients will save the weights for each client INSIDE a round\n",
    "    initial_weights = model.get_weights()\n",
    "    weights_clients = [[initial_weights] for _ in range(len(clients))]\n",
    "    # To save validation metrics INSIDE a round\n",
    "    loss_val_clients = []\n",
    "    acc_val_clients = []\n",
    "    AUC_val_clients = []\n",
    "    \n",
    "    for idx in range(len(clients)):\n",
    "        # The model is trained with the weights (initial in the first round or averaged later)\n",
    "        model.set_weights(weights_clients[idx][-1])\n",
    "        X_train_np = np.array(data_dict[idx]['X_train'])\n",
    "        y_train_np = np.array(data_dict[idx]['y_train'])\n",
    "        history = model.fit(X_train_np,y_train_np, epochs=38, batch_size=14, validation_split=0.15, verbose=0)\n",
    "        \n",
    "        # The new weights are added to the list stored at position i of weights_clients\n",
    "        weights_clients[clients[idx]-1].append(model.get_weights())\n",
    "    \n",
    "        # Add those weights to a list stored in position 'j', \n",
    "        # where 'j' represents all the clients with whom client 'i' communicates\n",
    "        weights_clients[clients_send[idx]-1].append(model.get_weights())\n",
    "        loss_val_clients.append(history.history['val_loss'][-1])\n",
    "        acc_val_clients.append(history.history['val_accuracy'][-1])\n",
    "        AUC_val_clients.append(history.history['val_auc'][-1])\n",
    "        \n",
    "    # Now there are two sets of weights per position in weights_clients. \n",
    "    # weights_clients = [[weights_clients1, weights_clients5], [weights_clients2, weights_clients1], \n",
    "    # [weights_clients3, weights_clients2], [weights_clients4, weights_clients3],\n",
    "    # [weights_clients5, weights_clients5]]\n",
    "    \n",
    "    # Averaging them will provide one set of weights again for each position in weights_clients.\n",
    "    for idx in range(len(clients)):\n",
    "        # Calculating the lengths\n",
    "        samples_len = [len(data_dict[clients[idx]-1]['X_train']),len(data_dict[clients_send[idx]-1]['X_train'])]\n",
    "        # Aggregate the weights obtained with each client using an aggregation function that performs a weighted average:\n",
    "        avg_weights = ave_weights(samples_len,weights_clients[idx]) # use of aggregation function\n",
    "        weights_clients[idx] = [avg_weights]\n",
    "        model.set_weights(avg_weights)\n",
    "        X_test_np = np.array(data_dict[idx]['X_test'])\n",
    "        y_test_np = np.array(data_dict[idx]['y_test'])\n",
    "        score_test = model.evaluate(X_test_np, y_test_np, verbose = 0)\n",
    "        # Saving metrics for validation:\n",
    "        loss_val_round[idx].append(loss_val_clients[idx])\n",
    "        acc_val_round[idx].append(acc_val_clients[idx])\n",
    "        AUC_val_round[idx].append(AUC_val_clients[idx])\n",
    "        # Saving metrics for test:\n",
    "        loss_test_round[idx].append(score_test[0])\n",
    "        acc_test_round[idx].append(score_test[1])\n",
    "        AUC_test_round[idx].append(score_test[2])\n",
    "    if i % 5 == 0: # Save weights_clients each 5 rounds (less storage space)\n",
    "        save_weights.append(weights_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549389a-9452-424d-902b-809390026553",
   "metadata": {},
   "source": [
    "Now, let's plot the evolution (in rounds) of validation metrics per client for the RAR architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574593b2-acbb-412f-9e8a-8d2fc924a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val_round_np = np.array(loss_val_round)\n",
    "# Create a figure with three subplots (one row and three columns)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8, 12))\n",
    "\n",
    "# Iterate over the lists of loss, accuracy, and AUC of each client\n",
    "for i, group in enumerate(groups):\n",
    "    name = group[0]\n",
    "    client_loss = loss_val_round_np[i]\n",
    "    client_accuracy = acc_val_round[i]\n",
    "    client_auc = AUC_val_round[i]\n",
    "    \n",
    "    # Subplot 1: Loss Curve\n",
    "    ax1.plot(range(1, n_times + 1), client_loss / max(client_loss), label= f'Client {i+1}: {name}', alpha=0.7)\n",
    "\n",
    "    # Subplot 2: Accuracy Curve\n",
    "    ax2.plot(range(1, n_times + 1), client_accuracy, label= f'Client {i+1}: {name}', alpha=0.7)\n",
    "\n",
    "    # Subplot 3: AUC Curve\n",
    "    ax3.plot(range(1, n_times + 1), client_auc, label= f'Client {i+1}: {name}', alpha=0.7)\n",
    "\n",
    "# Add labels and titles to subplots\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Normalized Average Loss')\n",
    "ax1.set_title('Normalized Average Loss (validation)')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_xlabel('Round')\n",
    "ax2.set_ylabel('Average Accuracy')\n",
    "ax2.set_title('Normalized Average Accuracy (validation)')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "ax3.set_xlabel('Round')\n",
    "ax3.set_ylabel('AUC')\n",
    "ax3.set_title('Area Under Curve (AUC - validation)')\n",
    "ax3.grid()\n",
    "ax3.legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14b98c-8592-4c19-8ed3-f1425769e280",
   "metadata": {},
   "source": [
    "Plotting the evolution of test metrics per client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbfcf17-ada2-4bf4-beec-d711d8adb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test_round_np = np.array(loss_test_round)\n",
    "# Create a figure with three subplots (one row and three columns)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8, 12))\n",
    "\n",
    "# Iterate over the lists of loss, accuracy, and AUC of each client\n",
    "for i, group in enumerate(groups):\n",
    "    client_loss = loss_test_round_np[i]\n",
    "    client_accuracy = acc_test_round[i]\n",
    "    client_auc = AUC_test_round[i]\n",
    "    \n",
    "    # Subplot 1: Loss Curve\n",
    "    ax1.plot(range(1, n_times + 1), client_loss / max(client_loss), label= f'Client {i+1}: {name}', alpha=0.7)\n",
    "\n",
    "    # Subplot 2: Accuracy Curve\n",
    "    ax2.plot(range(1, n_times + 1), client_accuracy, label= f'Client {i+1}: {name}', alpha=0.7)\n",
    "\n",
    "    # Subplot 3: AUC Curve\n",
    "    ax3.plot(range(1, n_times + 1), client_auc, label= f'Client {i+1}: {name}', alpha=0.7)\n",
    "\n",
    "# Add labels and titles to subplots\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Normalized Average Loss')\n",
    "ax1.set_title('Normalized Average Loss (test)')\n",
    "#ax1.set(xlim=(0, 25))\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_xlabel('Round')\n",
    "ax2.set_ylabel('Average Accuracy')\n",
    "ax2.set_title('Normalized Average Accuracy (test)')\n",
    "# ax2.set(xlim=(0, 25))\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "ax3.set_xlabel('Round')\n",
    "ax3.set_ylabel('AUC')\n",
    "ax3.set_title('Area Under Curve (AUC)')\n",
    "# ax3.set(xlim=(0, 25))\n",
    "ax3.grid()\n",
    "ax3.legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b469a-983d-4f77-8920-050cedeef620",
   "metadata": {},
   "source": [
    "Saving important lists for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec931bbe-8d96-4b71-8b5e-118babf74816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the lists of lists into a dictionary\n",
    "data_to_save = {\n",
    "    'acc_val_round': acc_val_round,\n",
    "    'loss_val_round': loss_val_round,\n",
    "    'AUC_val_round': AUC_val_round,\n",
    "    'acc_test_round': acc_test_round,\n",
    "    'loss_test_round': loss_test_round,\n",
    "    'AUC_test_round': AUC_test_round,\n",
    "    'save_weights': save_weights\n",
    "}\n",
    "\n",
    "# Save the combined data using Pickle\n",
    "with open('results_Test_RAR_v3_validation.pkl', 'wb') as file:\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a3b8e-9fab-4fb1-978a-f5723bbc839c",
   "metadata": {},
   "source": [
    "Calculation of the weight divergence between pairs of clients for rounds 70 or 80, where loss metric converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b681c-99ed-44fb-adcf-b0edc5b378d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_divergence(weights_m, weights_n):\n",
    "    \"\"\"\n",
    "    Compute the weight divergence between two sets of weights.\n",
    "\n",
    "    :param list weights_m: Set of weights for model m\n",
    "    :param list weights_n: Set of weights for model n\n",
    "    :return: Weight divergence between the two sets of weights\n",
    "    \"\"\"\n",
    "    # Flatten the weights arrays to 1-dimensional arrays\n",
    "    flat_weights_m = np.concatenate([w.ravel() for w in weights_m])\n",
    "    flat_weights_n = np.concatenate([w.ravel() for w in weights_n])\n",
    "\n",
    "    # Compute the Euclidean norm of the difference between weights_m and weights_n\n",
    "    norm_diff = np.linalg.norm(flat_weights_m - flat_weights_n)\n",
    "\n",
    "    # Compute the Euclidean norm of weights_n\n",
    "    norm_n = np.linalg.norm(flat_weights_n)\n",
    "\n",
    "    # Calculate the weight divergence\n",
    "    divergence = norm_diff / norm_n\n",
    "\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb488a-c1c3-4119-860a-c661f2ff9ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate weight divergence for all combinations of pairs of clients\n",
    "divergences = []\n",
    "for pair in itertools.combinations(range(5), 2):\n",
    "    client_a = pair[0]\n",
    "    client_b = pair[1]\n",
    "    divergence = weight_divergence(save_weights[14][client_a][0], save_weights[14][client_b][0])\n",
    "    divergences.append({'Pair of Clients': f'{client_a + 1}-{client_b + 1}', 'Weight Divergence': divergence})\n",
    "\n",
    "# Creating the table\n",
    "divergences_df = pd.DataFrame(divergences)\n",
    "divergences_df['Weight Divergence'] = divergences_df['Weight Divergence'].apply(lambda x: f'{x:.2f}')\n",
    "divergences_df.index += 1\n",
    "weight_divergence_table = divergences_df.style.set_caption('Weight Divergence in Round 70')\n",
    "\n",
    "# Display the table\n",
    "display(weight_divergence_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedfa0a-10b0-44bc-b88e-33037391b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weight divergence for all combinations of pairs of clients\n",
    "divergences = []\n",
    "for pair in itertools.combinations(range(5), 2):\n",
    "    client_a = pair[0]\n",
    "    client_b = pair[1]\n",
    "    divergence = weight_divergence(save_weights[16][client_a][0], save_weights[16][client_b][0])\n",
    "    divergences.append({'Pair of Clients': f'{client_a + 1}-{client_b + 1}', 'Weight Divergence': divergence})\n",
    "\n",
    "# Creating the table\n",
    "divergences_df = pd.DataFrame(divergences)\n",
    "divergences_df['Weight Divergence'] = divergences_df['Weight Divergence'].apply(lambda x: f'{x:.2f}')\n",
    "divergences_df.index += 1\n",
    "weight_divergence_table = divergences_df.style.set_caption('Weight Divergence in Round 80')\n",
    "\n",
    "# Display the table\n",
    "display(weight_divergence_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
