{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode Fixed-GL\n",
    "Set random seed  \n",
    "Create a variable for the maximum number of rounds\n",
    " \n",
    "Create an empty list for weighted average loss for each round  \n",
    "Create an empty list for weighted average accuracy for each round  \n",
    "Create a variable 'n' with a value of 0  \n",
    "Create a list to save the initial weights (new per round) \n",
    "Create a list of clients   \n",
    "Randomize the list of clients\n",
    "\n",
    "For each round 'i' up to the maximum number of rounds:\n",
    "- For index and client of list of clients:\n",
    "    - If 'n' is equal to 0:\n",
    "        - Fit the first model\n",
    "        - Save the model's weights for the next round\n",
    "        - Set 'n' to 1\n",
    "    - If 'n' is not equal to 0:\n",
    "        - Set initial weights\n",
    "        - Fit the new model\n",
    "        - Save the model's weights for the calculation of the weighted average\n",
    "        - Calculate the lengths of data points of the previous and new iteration and store them in a list\n",
    "        - Calculate the weighted average of the previous and new weights based on the lengths of data points\n",
    "        - Save the weighted average for the next round\n",
    "    - If last client of list of clients:\n",
    "        - Set weighted weights\n",
    "        - Fit the model with the weighted weights\n",
    "- Save the average loss for each round\n",
    "- Save the average accuracy for each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zQt1rPZj98cl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:22:25.243018: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 17:22:25.430206: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import random as rn\n",
    "rn.seed(0)\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "bkDlku9jUKP5",
    "outputId": "c3f9312e-25f6-4a8c-e900-100b0b94da2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "XzzTh5Y6NK7J",
    "outputId": "1e90158e-affe-4fb7-d9e1-0864090891af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>ST_Slope_Down</th>\n",
       "      <th>ST_Slope_Flat</th>\n",
       "      <th>ST_Slope_Up</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>ChestPainType_ATA</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>...</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>140.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>160.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>130.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>138.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54</td>\n",
       "      <td>150.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>110.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>144.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "      <td>130.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>138.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex_F  Sex_M  ST_Slope_Down  ST_Slope_Flat  ST_Slope_Up  \\\n",
       "0      0.0    1.0            0.0            0.0          1.0   \n",
       "1      1.0    0.0            0.0            1.0          0.0   \n",
       "2      0.0    1.0            0.0            0.0          1.0   \n",
       "3      1.0    0.0            0.0            1.0          0.0   \n",
       "4      0.0    1.0            0.0            0.0          1.0   \n",
       "..     ...    ...            ...            ...          ...   \n",
       "913    0.0    1.0            0.0            1.0          0.0   \n",
       "914    0.0    1.0            0.0            1.0          0.0   \n",
       "915    0.0    1.0            0.0            1.0          0.0   \n",
       "916    1.0    0.0            0.0            1.0          0.0   \n",
       "917    0.0    1.0            0.0            0.0          1.0   \n",
       "\n",
       "     ChestPainType_ASY  ChestPainType_ATA  ChestPainType_NAP  \\\n",
       "0                  0.0                1.0                0.0   \n",
       "1                  0.0                0.0                1.0   \n",
       "2                  0.0                1.0                0.0   \n",
       "3                  1.0                0.0                0.0   \n",
       "4                  0.0                0.0                1.0   \n",
       "..                 ...                ...                ...   \n",
       "913                0.0                0.0                0.0   \n",
       "914                1.0                0.0                0.0   \n",
       "915                1.0                0.0                0.0   \n",
       "916                0.0                1.0                0.0   \n",
       "917                0.0                0.0                1.0   \n",
       "\n",
       "     ChestPainType_TA  ExerciseAngina_N  ...  RestingECG_LVH  \\\n",
       "0                 0.0               1.0  ...             0.0   \n",
       "1                 0.0               1.0  ...             0.0   \n",
       "2                 0.0               1.0  ...             0.0   \n",
       "3                 0.0               0.0  ...             0.0   \n",
       "4                 0.0               1.0  ...             0.0   \n",
       "..                ...               ...  ...             ...   \n",
       "913               1.0               1.0  ...             0.0   \n",
       "914               0.0               1.0  ...             0.0   \n",
       "915               0.0               0.0  ...             0.0   \n",
       "916               0.0               1.0  ...             1.0   \n",
       "917               0.0               1.0  ...             0.0   \n",
       "\n",
       "     RestingECG_Normal  RestingECG_ST  Age  RestingBP  Cholesterol  FastingBS  \\\n",
       "0                  1.0            0.0   40      140.0        289.0          0   \n",
       "1                  1.0            0.0   49      160.0        180.0          0   \n",
       "2                  0.0            1.0   37      130.0        283.0          0   \n",
       "3                  1.0            0.0   48      138.0        214.0          0   \n",
       "4                  1.0            0.0   54      150.0        195.0          0   \n",
       "..                 ...            ...  ...        ...          ...        ...   \n",
       "913                1.0            0.0   45      110.0        264.0          0   \n",
       "914                1.0            0.0   68      144.0        193.0          1   \n",
       "915                1.0            0.0   57      130.0        131.0          0   \n",
       "916                0.0            0.0   57      130.0        236.0          0   \n",
       "917                1.0            0.0   38      138.0        175.0          0   \n",
       "\n",
       "     MaxHR  Oldpeak  HeartDisease  \n",
       "0      172      0.0             0  \n",
       "1      156      1.0             1  \n",
       "2       98      0.0             0  \n",
       "3      108      1.5             1  \n",
       "4      122      0.0             0  \n",
       "..     ...      ...           ...  \n",
       "913    132      1.2             1  \n",
       "914    141      3.4             1  \n",
       "915    115      1.2             1  \n",
       "916    174      0.0             1  \n",
       "917    173      0.0             0  \n",
       "\n",
       "[918 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading processed and curated dataset:\n",
    "dfHeart = pd.read_csv(\"/srv/heart_ConditionalMeanImputation.csv\")\n",
    "dfHeart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MZnVjcdJYDIp"
   },
   "outputs": [],
   "source": [
    "# X_clients = 9 # number of clients\n",
    "# data_dict = {}\n",
    "\n",
    "# for i in range(X_clients):\n",
    "#   portion = 1/(X_clients)*len(dfHeart)\n",
    "#   X = dfHeart.loc[i*portion:(i+1)*portion,:].drop(['HeartDisease'], axis=1)\n",
    "#   y = dfHeart.loc[i*portion:(i+1)*portion,:]['HeartDisease']\n",
    "#   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "#   client_data = {\n",
    "#       'X_train': X_train,\n",
    "#       'X_test': X_test,\n",
    "#       'y_train': y_train,\n",
    "#       'y_test': y_test\n",
    "#   }\n",
    "#   data_dict[f'client_{i+1}'] = client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sLxPo0OnS1hD"
   },
   "outputs": [],
   "source": [
    "dfHeart[\"Name\"]=\"name\"\n",
    "# Estimated (from raw data) slices for each hospital\n",
    "dfHeart.loc[0:293,\"Name\"] = \"hung\" # Hungarian Institute of Cardiology, Budapest\n",
    "# Author: Andras Janosi, M.D.\n",
    "dfHeart.loc[293:477,\"Name\"] = \"swit\" # University Hospital (Zurich and Basel) Switzerland\n",
    "# Author: William Steinbrunn, M.D. and Matthias Pfisterer, M.D respectively.\n",
    "dfHeart.loc[477:615,\"Name\"] = \"long\" # V.A. Medical Center, Long Beach\n",
    "# Author: Robert Detrano, M.D., Ph.D.\n",
    "dfHeart.loc[615:766,\"Name\"] = \"stat\" # Statlog dataset (confidential source)\n",
    "# Author: \"King RD. Department of Statistics and Modelling Science, University of Strathclyde, Glasgow\"\n",
    "dfHeart.loc[766:918,\"Name\"] = \"clev\" # Cleveland Clinic Foundation\n",
    "# Author: Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OzJuqZHlRTlO"
   },
   "outputs": [],
   "source": [
    "# Each group is divided into train and test groups and scaled\n",
    "# Creation of data_dict dictionary\n",
    "data_dict = {}\n",
    "groups = dfHeart.groupby('Name')\n",
    "\n",
    "for name, group in groups:\n",
    "    X = group.drop(['HeartDisease', 'Name'], axis=1)\n",
    "    y = group['HeartDisease']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "    # Judith: escalamos los datos\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # Filling data_dict dictionary\n",
    "    data_dict[name] = {\n",
    "      'X_train': X_train,\n",
    "      'X_test': X_test,\n",
    "      'y_train': y_train,\n",
    "      'y_test': y_test\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nkzx40uITwj5",
    "outputId": "9095cd4f-8727-4b9a-c2bc-4c893457db7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , ..., 0.        , 0.51351351,\n",
       "        0.09756098],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.0990991 ,\n",
       "        0.02439024],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.27927928,\n",
       "        0.02439024],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.        , ..., 1.        , 0.45945946,\n",
       "        0.02439024],\n",
       "       [0.        , 1.        , 0.        , ..., 1.        , 0.3963964 ,\n",
       "        0.26829268],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.72972973,\n",
       "        0.02439024]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[\"long\"]['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q11bVG7aRuoG",
    "outputId": "fbde4ce4-3499-4430-bbc8-d36a49628b34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clev', 'hung', 'long', 'stat', 'swit'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TiRb9YCCoFBy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:22:27.930432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:27.936496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:27.938057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:27.940421: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 17:22:27.949941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:27.951651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:27.953250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:29.264456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:29.266247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:29.267805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 17:22:29.269358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12315 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:0a.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#Creating neural network architecture\n",
    "shape = data_dict[list(data_dict.keys())[0]]['X_train'].shape[1]\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=256, input_shape=(shape,), activation=\"relu\", kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss=\"binary_crossentropy\", \n",
    "                metrics=[\"accuracy\", \"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OGNBFINr4Mbl"
   },
   "outputs": [],
   "source": [
    "def ave_weights(n_i,listOfWeights):\n",
    "    \"\"\"\n",
    "    Aggregation function\n",
    "\n",
    "   :param list n_i: Number of samples for each client\n",
    "   :param list listOfWeights: Weights for each client\n",
    "   :return: Final weighted average for global model\n",
    "    \"\"\"\n",
    "    N = sum(n_i) # total number of samples of all clients\n",
    "    # initial weights of global model, set to zero\n",
    "    ave_weights = listOfWeights[0]\n",
    "    ave_weights = [i * 0 for i in ave_weights]\n",
    "    # loop whose range is number of clients\n",
    "    for j in range(len(n_i)):\n",
    "        # receive weights from clients\n",
    "        rec_weight = listOfWeights[j]\n",
    "        # multiply the client weights by number of local data samples in client local data\n",
    "        rec_weight =  [i * n_i[j] for i in rec_weight]\n",
    "        # divide the weights by total number of samples of all clients\n",
    "        rec_weight =  [i / N for i in rec_weight]\n",
    "        # sum the weights of new client with the prior\n",
    "        ave_weights = [x + y for x, y in zip(ave_weights,rec_weight)]\n",
    "    return ave_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(n_i, metric_list):\n",
    "    \"\"\"\n",
    "    Calculate the weighted mean of an evaluation metric.\n",
    "\n",
    "    :param list n_i: Number of samples for each client\n",
    "    :param list metric_list: List of evaluation metric values for each client\n",
    "    :return: Weighted mean of the evaluation metric\n",
    "    \"\"\"\n",
    "    total_samples = sum(n_i)  # Total number of samples of all clients\n",
    "    weighted_metric_sum = 0.0\n",
    "\n",
    "    for j in range(len(n_i)):\n",
    "        weighted_metric_sum += metric_list[j] * n_i[j]\n",
    "\n",
    "    weighted_mean = weighted_metric_sum / total_samples\n",
    "    return weighted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Select a maximum number of rounds, leaving a validation set for each dataset.\n",
    "-  Analyze the error/accuracy of the resulting model for each round in the validation of each dataset.\n",
    "-  Calculate the weighted average of the error/accuracy for each round. In other words, for round i, we will have different error and accuracy values for each dataset. You can aggregate them (weighted based on the number of data points) and obtain the weighted average of the error and/or accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:22:33.512012: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x2d826cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-14 17:22:33.512126: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-08-14 17:22:33.528811: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-14 17:22:33.813405: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "#Set random seed\n",
    "rn.seed(42)\n",
    "\n",
    "# Maximum number of rounds n_times\n",
    "n_times = 300\n",
    "\n",
    "acc_val_round = [] # accuracy validation of a round\n",
    "loss_val_round = [] # loss validation of a round\n",
    "\n",
    "# Initial parameters previous random client list generation\n",
    "my_list = list(data_dict.keys())\n",
    "# Changing the order of one-to-one communication, the same order for each round\n",
    "random.shuffle(my_list)\n",
    "# Initial weights previous rounds, it will be updated for each client on each round\n",
    "saveModels = [model.get_weights()]*len(my_list)\n",
    "\n",
    "# Rounds of the randomized gossip learning\n",
    "for i in range(1,n_times+1):\n",
    "    \n",
    "    # Each round of the fixed gossip learning      \n",
    "    for idx, name in enumerate(my_list):\n",
    "        # At the beginning of the first round. The list \"acc_val_round\" is empty \n",
    "        if not acc_val_round:\n",
    "            # ------------------------------------------\n",
    "            # TRAINING FIRST CLIENT FOR THE FIRST TIME:\n",
    "            X_train_np = np.array(data_dict[name]['X_train'])\n",
    "            y_train_np = np.array(data_dict[name]['y_train'])\n",
    "            history = model.fit(X_train_np,y_train_np, epochs=14, \n",
    "                                batch_size=38, validation_split=0.15, verbose=0)\n",
    "            saveModels[idx] = model.get_weights()\n",
    "            # ------------------------------------------\n",
    "        else:\n",
    "            # ------------------------------------------\n",
    "            # TRAINING CLIENT N:\n",
    "            # Setting initial weights (of the model updating in the previous round)\n",
    "            model.set_weights(saveModels[idx])\n",
    "            # Train client:\n",
    "            X_train_np = np.array(data_dict[name]['X_train'])\n",
    "            y_train_np = np.array(data_dict[name]['y_train'])\n",
    "            history = model.fit(X_train_np,y_train_np, epochs=14, \n",
    "                                batch_size=38, validation_split=0.15, verbose=0)\n",
    "            # Obtain client weights:\n",
    "            saveModels[idx] = model.get_weights()\n",
    "            # ------------------------------------------\n",
    "            # AVERAGING WITH WEIGHT OF N-1 CLIENT:\n",
    "            # Calculating the lengths of the names of the previous and current iteration\n",
    "            samples_len = [len(data_dict[my_list[idx-1]]['X_train']),len(data_dict[name]['X_train'])]\n",
    "            # Aggregate the weights obtained with each client using an aggregation function that performs a weighted average:\n",
    "            avg_weights = ave_weights(samples_len,[saveModels[idx-1],saveModels[idx]]) # use of aggregation function\n",
    "            # ------------------------------------------\n",
    "            # UPDATING MODEL:\n",
    "            saveModels[idx] = avg_weights\n",
    "            # ------------------------------------------\n",
    "        # ----------------------------------------------\n",
    "        # AT THE FINAL OF THE ROUND, OBTAIN VALIDATION METRICS\n",
    "        if name == my_list[-1]:\n",
    "            model.set_weights(saveModels[idx])\n",
    "            X_train_np = np.array(data_dict[name]['X_train'])\n",
    "            y_train_np = np.array(data_dict[name]['y_train'])\n",
    "            history = model.fit(X_train_np,y_train_np, epochs=14, \n",
    "                                batch_size=38, validation_split=0.15, verbose=0)\n",
    "            # Save the accuracy and loss at the final of the round\n",
    "            acc_val_round.append(history.history['val_accuracy'][-1])\n",
    "            loss_val_round.append(history.history['val_loss'][-1])\n",
    "        # ----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have that weighted average for validation, select the optimal number of rounds that need to be repeated for the FL architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "loss_val_round_np = np.array(loss_val_round)\n",
    "\n",
    "# Plot the average loss on the left y-axis\n",
    "\n",
    "ax1.plot(range(1, n_times + 1), loss_val_round_np/max(loss_val_round_np), label='Average Loss', color='blue')\n",
    "# Explica que aquí no es la loss lo que pones en el plot, si no la versión normalizada\n",
    "ax1.set_xlabel('Round')\n",
    "ax1.set_ylabel('Normalized Average Loss')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# Create a twin axes on the right side\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(range(1, n_times + 1), acc_val_round, label='Average Accuracy', color='red')\n",
    "ax2.set_ylabel('Average Accuracy')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Set the title\n",
    "plt.title('Normalized Average Loss and Average Accuracy (validation)')\n",
    "\n",
    "# Add legends\n",
    "lines = [ax1.get_lines()[0], ax2.get_lines()[0]]\n",
    "plt.legend(lines, [line.get_label() for line in lines])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best equilibrium between acc_val_round and loss_val_round is found at the round 32\n",
    "max_acc_round = np.argmax(acc_val_round) + 1\n",
    "max_acc_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_acc = acc_val_round[max_acc_round-1]\n",
    "optim_loss = loss_val_round[max_acc_round-1]\n",
    "print(\" optim_acc:\",optim_acc)\n",
    "print(\" optim_loss:\",optim_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process without using validation (i.e., now we do not leave data aside for validation since validation has already been done previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=256, input_shape=(shape,), activation=\"relu\", kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss=\"binary_crossentropy\", \n",
    "                metrics=[\"accuracy\", \"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Maximum number of rounds n_times\n",
    "n_times = max_acc_round\n",
    "\n",
    "# Initial parameters previous random client list generation\n",
    "my_list = list(data_dict.keys())\n",
    "# Changing the order of one-to-one communication, the same order for each round\n",
    "random.shuffle(my_list)\n",
    "# Initial weights previous rounds, it will be updated for each client on each round\n",
    "saveModels = [model.get_weights()]*len(my_list)\n",
    "\n",
    "# Rounds of the randomized gossip learning\n",
    "for i in range(1,n_times+1):\n",
    "    \n",
    "    # Each round of the fixed gossip learning      \n",
    "    for idx, name in enumerate(my_list):\n",
    "        # At the beginning of the first round. The list \"acc_val_round\" is empty \n",
    "        if not acc_val_round:\n",
    "            # ------------------------------------------\n",
    "            # TRAINING FIRST CLIENT FOR THE FIRST TIME:\n",
    "            X_train_np = np.array(data_dict[name]['X_train'])\n",
    "            y_train_np = np.array(data_dict[name]['y_train'])\n",
    "            history = model.fit(X_train_np,y_train_np, epochs=14, \n",
    "                                batch_size=38, verbose=0)\n",
    "            saveModels[idx] = model.get_weights()\n",
    "            # ------------------------------------------\n",
    "        else:\n",
    "            # ------------------------------------------\n",
    "            # TRAINING CLIENT N:\n",
    "            # Setting initial weights (of the model updating in the previous round)\n",
    "            model.set_weights(saveModels[idx])\n",
    "            # Train client:\n",
    "            X_train_np = np.array(data_dict[name]['X_train'])\n",
    "            y_train_np = np.array(data_dict[name]['y_train'])\n",
    "            history = model.fit(X_train_np,y_train_np, epochs=14, \n",
    "                                batch_size=38, validation_split=0.15, verbose=0)\n",
    "            # Obtain client weights:\n",
    "            saveModels[idx] = model.get_weights()\n",
    "            # ------------------------------------------\n",
    "            # AVERAGING WITH WEIGHT OF N-1 CLIENT:\n",
    "            # Calculating the lengths of the names of the previous and current iteration\n",
    "            samples_len = [len(data_dict[my_list[idx-1]]['X_train']),len(data_dict[name]['X_train'])]\n",
    "            # Aggregate the weights obtained with each client using an aggregation function that performs a weighted average:\n",
    "            avg_weights = ave_weights(samples_len,[saveModels[idx-1],saveModels[idx]]) # use of aggregation function\n",
    "            # ------------------------------------------\n",
    "            # UPDATING MODEL:\n",
    "            saveModels[idx] = avg_weights\n",
    "            # ------------------------------------------\n",
    "model.save('model_initialTest_GL_fixed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aquí estás volviendo a entrenar, tendrías que evaluar directamente los resultados en test con el último modelo tras agregar (y mirar el train de cada cliente)\n",
    "# # history = model.fit(X_train,y_train, epochs=14, batch_size=38, verbose=0)\n",
    "# for name in data_dict.keys():\n",
    "#     print(f'Client: {name}')\n",
    "#     score_train = model.evaluate(data_dict[name]['X_train'],data_dict[name]['y_train'], verbose = 0)\n",
    "#     print(f'Train loss: {score_train[0]}')\n",
    "#     print(f'Train accuracy: {score_train[1]}')\n",
    "#     print(f'Train AUC: {score_train[2]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in data_dict.keys():\n",
    "    print(f'Client: {name}')\n",
    "    score_test = model.evaluate(data_dict[name]['X_test'],data_dict[name]['y_test'], verbose = 0)\n",
    "    print(f'Test loss: {score_test[0]}')\n",
    "    print(f'Test accuracy: {score_test[1]}')\n",
    "    print(f'Test AUC: {score_test[2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it overfits because I trained a model with the same data points when cross-validated."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
